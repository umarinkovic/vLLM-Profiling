#!/usr/bin/env python3

"""
orhcestrator.py - script for automatically running all models in .config/models.yaml on all
gpus listed in gpus.yaml 
"""

import argparse
from collections import defaultdict, deque
import subprocess
from pathlib import Path
import yaml
import os

PROJECT_ROOT = Path(__file__).parent.parent.parent

def prepare_tokens():
    tokens_path = PROJECT_ROOT / ".config" / "tokens.yaml"
    if not tokens_path.exists():
        print(f"No {tokens_path} file found.")
        return
    with tokens_path.open("r") as f:
        tokens = yaml.safe_load(f)["tokens"]
    for key, value in tokens.items():
        os.environ[key] = value


def parse_gpus():
    file_path = PROJECT_ROOT / ".config" / "gpus.yaml"
    if not file_path.exists():
        subprocess.run(["generate_gpu_yaml.sh"], check=True)
    with file_path.open("r") as f:
        return [gpu for gpu in yaml.safe_load(f)["gpus"] if not gpu.get("disabled", False)]
    

def parse_models(models_filter = None):
    file_path = PROJECT_ROOT / "yaml" / "models.yaml"
    if not file_path.exists():
        raise FileNotFoundError(f"No {file_path} found.")
    with open(file_path, "r") as f:
        return [model for model in yaml.safe_load(f)[f"models"] if not models_filter or model["name"] in models_filter]
    
def run(docker_image, num_procs, script, duration, models_filter):
    gpus = parse_gpus()
    models = parse_models(models_filter)

    # create a map {device_name -> queue({model_to_run, environment})}
    device_task_queue_map = defaultdict(deque)
    for gpu in gpus:
        for model in models:
            if gpu["name"] not in model.get("disabled_on", []):
                # environment is generated by taking the env dictionary from the model and superimposing the env dictionary of the gpu
                device_task_queue_map[gpu["device"]].append({"model": model, "env": model.get('env', {}) | gpu.get('env', {})})

    device_to_name_map = {gpu["device"] : gpu["name"] for gpu in gpus}
    prepare_tokens()

    # TODO: implement parallelism, currently writing single threaded but will be expanded later
    # to include the possibility of starting multiple profiling tasks on different GPUs
    devices = list(device_task_queue_map.keys())
    while any(device_task_queue_map.values()):
        for device in devices:
            task_queue = device_task_queue_map[device]
            if task_queue:
                task = model = task_queue.popleft()
                model = task["model"]
                env = task["env"]
                script_args = ["--model", model['name'], "--script", model['script'],"--duration", str(duration)]
                subprocess.call([
                    "scripts/host/docker_tool.py",
                    "run", 
                    # TODO: enable correct parsing of --image-name argument (gets forwarded as remainder arg to script for some reason?)
                    # "--image-name",
                    # docker_image,
                    "--device-name",
                    device_to_name_map[device],
                    "--device",
                    device,
                    "--script",
                    script, 
                    "--",
                    ] 
                    + 
                    script_args, 
                    env=os.environ | env
                )


def main():
    parser = argparse.ArgumentParser("Orchestrate profiling of given models on available AMD GPUs")
    parser.add_argument("--docker-image", help = "Docker image on which to run vllm",
                        default = "hyoon11/vllm-dev:20260121_43_py3.12_torch2.9_triton3.5_navi_upstream_6a09612_ubuntu24.04")
    parser.add_argument("--num-procs", 
                        help="""One GPU always runs one profiling task at a time.
                        On a system with multiple GPUs, we can choose to run multiple 
                        profiling tasks (1 for each GPU maximum), if resource exhaustion
                        is not a problem (RAM being the main concern)""",
                        default=1)
    parser.add_argument("--script", 
                        help="Name of the script to run inside the containers.",
                        default="run_model.py")
    parser.add_argument("--duration",
                        help="Duration in seconds for which to run each model. (default=60s)",
                        default=60)
    parser.add_argument("models_filter",
                         help="Subset of models to run from the models.yaml file. If left empty, runs all models.",
                         nargs='*')
    
    args = parser.parse_args()

    run(args.docker_image, args.num_procs, args.script, args.duration, args.models_filter)


if __name__ == "__main__":
    main()